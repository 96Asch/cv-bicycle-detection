{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CV Bike Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96Asch/cv-bicycle-detection/blob/master/CV_Bike_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nGiIgx6PQR_i"
      },
      "cell_type": "markdown",
      "source": [
        "#Bike detection using a Mask RCNN pretrained model\n",
        "In this notebook we fine tune a Mask RCNN model, trained on the MSCOCO dataset, on our own created bike dataset. This dataset has already been pre-generated and is stored on a publicly accesible drive.\n",
        "\n",
        "You can run the notebook in sequence except for when downgrading Keras. More instruction on this will be below."
      ]
    },
    {
      "metadata": {
        "id": "ywU9FM5qn6Wx"
      },
      "cell_type": "markdown",
      "source": [
        "#Install required packages\n",
        "\n",
        "Here we clone the Mask RCNN repo and install the requirements from the given requirements.txt file"
      ]
    },
    {
      "metadata": {
        "id": "39Wtm53bG7xW"
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "  \n",
        "!git clone --quiet https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZY2h5buS9xb"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "\n",
        "!pip install -q PyDrive\n",
        "!pip install -r requirements.txt\n",
        "!pip install 'h5py<3.0.0'\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zZg68_koGU7"
      },
      "cell_type": "markdown",
      "source": [
        "#Download and extract dataset\n",
        "This downloads the COCO_Bikes dataset from Google Drive and extracts it into Mask_RCNN/dataset/"
      ]
    },
    {
      "metadata": {
        "id": "p3KxhAII1PDT"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "\n",
        "\n",
        "fileId = '14EAGfdRPSuIcrB3nmvXOfjtICFNALnMT'\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "if not os.path.exists('./dataset'):\n",
        "  os.makedirs('dataset')\n",
        "os.chdir('dataset')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "if not os.path.exists('./COCO_Bikes'):\n",
        "  fileName = fileId + '.zip'\n",
        "  downloaded = drive.CreateFile({'id': fileId})\n",
        "  downloaded.GetContentFile(fileName)\n",
        "  ds = ZipFile(fileName)\n",
        "  ds.extractall()\n",
        "  os.remove(fileName)\n",
        "  print('Extracted zip file ' + fileName)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the pretrained weights\n",
        "We fetch the MSCOCO pretrained weights of Mask RCNN\n"
      ],
      "metadata": {
        "id": "bn1XKNX_5cgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
      ],
      "metadata": {
        "id": "YA3ygqcriv1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Hjjc40apLpd"
      },
      "cell_type": "markdown",
      "source": [
        "#Configure the the Config and Dataset \n",
        "\n",
        "The Config class handles the configuration of the training.\n",
        "The Dataset class handles the import of our custom dataset"
      ]
    },
    {
      "metadata": {
        "id": "0SVHFnl5Hwe3"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import sys\n",
        "\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "\n",
        "sys.path.append(os.path.join(\"/root/Mask_RCNN\", \"samples/coco/\"))  # To find local version\n",
        "import coco\n",
        "\n",
        "class DelftBikesConfig(Config):\n",
        "    NAME = \"delft-bikes\"\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 1 + 1  # background + bike\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    LEARNING_RATE = 0.001\n",
        "  \n",
        "\n",
        "class COCODelftBikes(coco.CocoDataset):\n",
        "\n",
        "    def load(self, dataset_dir: str, subset: str) -> None:\n",
        "\n",
        "        path = os.path.join(dataset_dir, subset)\n",
        "        annotation = os.path.join(dataset_dir, f\"{subset}.json\")\n",
        "\n",
        "        coco = COCO(annotation)\n",
        "        class_ids = sorted(coco.getCatIds())\n",
        "        \n",
        "        image_ids = []\n",
        "        for id in class_ids:\n",
        "            image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "        # Remove duplicates\n",
        "        image_ids = list(set(image_ids))\n",
        "\n",
        "        self.add_class(\"coco\", 1, coco.loadCats(1)[0][\"name\"])\n",
        "        \n",
        "        \n",
        "          # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"coco\", image_id=i,\n",
        "                path=os.path.join(path, coco.imgs[i]['file_name']),\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=False)))\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "        Different datasets use different ways to store masks. This\n",
        "        function converts the different mask format to one format\n",
        "        in the form of a bitmap [height, width, instances].\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"coco\":\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
        "            return mask, np.ones([mask.shape[-1]], dtype=np.uint8)\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Check if the GPU is running"
      ],
      "metadata": {
        "id": "3gkpTG4K5pjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/CPU:0'\n",
        "\n",
        "print('Using: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "4jeOnZoElFXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "Here we begin training the model, by first enabling Tensorboard in order to visualize each loss later on."
      ],
      "metadata": {
        "id": "j-R3wQ2G53Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "vZHNelGFlUuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Re-install the correct version of Keras\n",
        "For some reason, the built-in version of Keras gets reset when using the tensorflow version command. So we install the correct version: 2.2.5.\n",
        "You must the install below and reset the runtime to change the versions. Then re-run the install cell and go back to the config and dataset and continue from there."
      ],
      "metadata": {
        "id": "6HFxbCsVCUkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.2.5\n",
        "!pip list | grep Keras"
      ],
      "metadata": {
        "id": "s1qemAvGJe-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pretrained model and begin training using the dataset and config files specified before.\n",
        "\n",
        "The amount of epochs can be changed in the train function below. For our experiment, we chose an epoch=100, however since it saves the weights after each epoch, you can stop training if tensorboard shows that the training and validation losses are diverging."
      ],
      "metadata": {
        "id": "7N2Tihblshzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VnGjVNjJR4hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "import mrcnn.model as modellib\n",
        "\n",
        "config = DelftBikesConfig()\n",
        "model_save_dir = os.path.join(\"/content\", \"exps\")\n",
        "pretrained_model_file = \"/root/mask_rcnn_coco.h5\"\n",
        "data_set_dir = \"/root/Mask_RCNN/dataset/COCO_Bikes\"\n",
        "\n",
        "with tf.device(device_name):\n",
        "    model = modellib.MaskRCNN(mode='training', config=config, model_dir=model_save_dir)\n",
        "\n",
        "assert os.path.exists(pretrained_model_file)\n",
        "    \n",
        "model.load_weights(pretrained_model_file, by_name=True, exclude=[ \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "dataset_train = COCODelftBikes()\n",
        "dataset_train.load(data_set_dir, 'train')\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = COCODelftBikes()\n",
        "dataset_val.load(data_set_dir, 'test')\n",
        "dataset_val.prepare()\n",
        "\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=100,\n",
        "            layers='all')"
      ],
      "metadata": {
        "id": "No4iRU46lXnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the losses using TensorBoard\n",
        "You should edit the path after --logdir and put the path to folder created by Mask RCNN which contains the tfevents file"
      ],
      "metadata": {
        "id": "MJG9ap27qs2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/exps"
      ],
      "metadata": {
        "id": "q_y1HGpHpHWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the ValBikes Dataset\n"
      ],
      "metadata": {
        "id": "uKByxqPOfuzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "\n",
        "fileId = \"1rTPgMfAB9T33utZZ34tIuqaHaZEg0byV\"\n",
        "\n",
        "if not os.path.exists('./dataset'):\n",
        "  os.makedirs('dataset')\n",
        "os.chdir('dataset')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "if not os.path.exists('./ValBikes'):\n",
        "  fileName = fileId + '.zip'\n",
        "  downloaded = drive.CreateFile({'id': fileId})\n",
        "  downloaded.GetContentFile(fileName)\n",
        "  ds = ZipFile(fileName)\n",
        "  ds.extractall()\n",
        "  os.remove(fileName)\n",
        "  print('Extracted zip file ' + fileName)"
      ],
      "metadata": {
        "id": "TpBSsSh9fy1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the two fine-tuned models\n",
        "Here we will retrieve the models that were fine-tuned on our augmented dataset, one model was trained with the default LR=0.001 and the other with a LR=0.0001"
      ],
      "metadata": {
        "id": "nYqa-r4tRAXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "\n",
        "maskLR0001_id = '1ClTIBdFRNMLb9utSMjOEGh7s9Uci76Y2'\n",
        "maskLR00001_id = '11VD9mWNN9YK8ZI4Rm2myDLRVAC1EUv_z'\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "if not os.path.exists('./maskLR0001.h5'):\n",
        "  downloaded = drive.CreateFile({'id': maskLR0001_id})\n",
        "  downloaded.GetContentFile('maskLR0001.h5')\n",
        "\n",
        "if not os.path.exists('./maskLR00001.h5'):\n",
        "  downloaded = drive.CreateFile({'id': maskLR00001_id})\n",
        "  downloaded.GetContentFile('maskLR00001.h5')"
      ],
      "metadata": {
        "id": "rGKxA438Q0bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run tests on the three models\n",
        "For these tests, we will calculate the mean average precision (mAP) and the mean intersection over union (mIOU). Luckily, Mask RCNN already provides us with these metrics in the utils file."
      ],
      "metadata": {
        "id": "edmbdX27Ty2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfigAllClass(coco.CocoConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 81\n",
        "\n",
        "class InferenceConfigBikeClass(coco.CocoConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "\n",
        "class ValBikes(coco.CocoDataset):\n",
        "\n",
        "    def load(self, dataset_dir: str) -> None:\n",
        "\n",
        "        path = os.path.join(dataset_dir, \"val\")\n",
        "        annotation = os.path.join(dataset_dir, \"valbikes.json\")\n",
        "\n",
        "        coco = COCO(annotation)\n",
        "        class_ids = sorted(coco.getCatIds())\n",
        "        \n",
        "        image_ids = []\n",
        "        for id in class_ids:\n",
        "            image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "        # Remove duplicates\n",
        "        image_ids = list(set(image_ids))\n",
        "\n",
        "        self.add_class(\"coco\", 1, coco.loadCats(1)[0][\"name\"])\n",
        "        \n",
        "          # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"coco\", image_id=i,\n",
        "                path=os.path.join(path, coco.imgs[i]['file_name']),\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "        Different datasets use different ways to store masks. This\n",
        "        function converts the different mask format to one format\n",
        "        in the form of a bitmap [height, width, instances].\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"coco\":\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
        "            return mask, np.ones([mask.shape[-1]], dtype=np.uint8)\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            print(\"empty mask\")\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)"
      ],
      "metadata": {
        "id": "U5NtA9G9WGVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import mrcnn.model as modellib\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "model_save_dir = os.path.join('/content', 'model')\n",
        "baseline_model_file = \"/root/mask_rcnn_coco.h5\"\n",
        "lr0001_model_file = \"/root/maskLR0001.h5\"\n",
        "lr00001_model_file = \"/root/maskLR00001.h5\"\n",
        "\n",
        "allClassConfig = InferenceConfigAllClass()\n",
        "bikeClassConfig = InferenceConfigBikeClass()\n",
        "\n",
        "with tf.device(device_name):\n",
        "    baseline = modellib.MaskRCNN(mode='inference', config=allClassConfig, model_dir=model_save_dir)\n",
        "    model_LR0001 = modellib.MaskRCNN(mode='inference', config=bikeClassConfig, model_dir=model_save_dir)\n",
        "    model_LR00001 = modellib.MaskRCNN(mode='inference', config=bikeClassConfig, model_dir=model_save_dir)\n",
        "\n",
        "baseline.load_weights(baseline_model_file, by_name=True)\n",
        "model_LR0001.load_weights(lr0001_model_file, by_name=True)\n",
        "model_LR00001.load_weights(lr00001_model_file, by_name=True)    \n"
      ],
      "metadata": {
        "id": "03DhQTKPUGHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax\n",
        "\n",
        "coco_class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n"
      ],
      "metadata": {
        "id": "nVAO88FNk0u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrcnn import utils\n",
        "\n",
        "data_set_dir = os.path.join(\"/root\", \"Mask_RCNN\", \"dataset\", \"ValBikes\")\n",
        "\n",
        "valbikes = ValBikes()\n",
        "valbikes.load(data_set_dir)\n",
        "valbikes.prepare()\n",
        "\n",
        "coco_dataset = coco.CocoDataset()\n",
        "coco_dataset.load_coco(os.path.join(\"/root\", \"Mask_RCNN\", \"dataset\", \"COCO\"), \"minival\", class_ids=[2], auto_download=False)\n",
        "coco_dataset.prepare()"
      ],
      "metadata": {
        "id": "0MOlriQ3Wy7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculate the APs and IOUs\n",
        "To change the test dataset, choose one of the datasets above to run evaluation"
      ],
      "metadata": {
        "id": "9oNbqwWbjR45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from mrcnn import visualize \n",
        "\n",
        "# Choose either coco_dataset or valbikes to run evaluation\n",
        "dataset = coco_dataset\n",
        "config = InferenceConfigAllClass()\n",
        "\n",
        "baseline_APs = []\n",
        "baseline_IOUs = []\n",
        "\n",
        "m1_APs = []\n",
        "m1_IOUs = []\n",
        "\n",
        "m2_APs = []\n",
        "m2_IOUs = []\n",
        "\n",
        "for image_id in tqdm(dataset.image_ids):\n",
        "\n",
        "    # Load image and ground truth data\n",
        "  image, _, gt_class_id, gt_bbox, gt_mask = \\\n",
        "      modellib.load_image_gt(dataset, config, image_id)\n",
        "\n",
        "  # Run object detection\n",
        "  def eval_model(model, image, is_bike_model=False):\n",
        "    r = model.detect([image], verbose=0)[0]\n",
        "\n",
        "    class_ids = r[\"class_ids\"]\n",
        "    if not is_bike_model:\n",
        "      class_ids = np.array([1 if id == 2 else id for id in class_ids])\n",
        "    AP, _, _, _ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                        r[\"rois\"], class_ids, r[\"scores\"], r['masks'], iou_threshold=0.5)\n",
        "    IOU = utils.compute_overlaps(r[\"rois\"], gt_bbox)\n",
        "    return AP, IOU\n",
        "\n",
        "\n",
        "  b_AP, b_iou = eval_model(baseline, image)\n",
        "  baseline_APs.append(b_AP)\n",
        "  baseline_IOUs.append(b_iou)\n",
        "\n",
        "  m1_AP, m1_iou = eval_model(model_LR0001, image, True)\n",
        "  m1_APs.append(m1_AP)\n",
        "  m1_IOUs.append(m1_iou)\n",
        "\n",
        "  m2_AP, m2_iou = eval_model(model_LR00001, image, True)\n",
        "  m2_APs.append(m2_AP)\n",
        "  m2_IOUs.append(m2_iou)\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "4X4nOt7NjHVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we take the ious calculated in the last step and discard all the empty IoUs (where the detector could not find a bicycle), replacing them with 0. We then take the mean of all ious to get our mIoU metric"
      ],
      "metadata": {
        "id": "jR7ZiyhpbCP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_mean(IOUs):\n",
        "  return np.mean([np.mean(iou) if iou.size else 0 for iou in IOUs])\n",
        "\n",
        "b_ious = iou_mean(baseline_IOUs)\n",
        "m1_ious = iou_mean(m1_IOUs)\n",
        "m2_ious = iou_mean(m2_IOUs)"
      ],
      "metadata": {
        "id": "Xx_r-yO8Y8dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import csv\n",
        "\n",
        "header = ['Model', 'mAP', \"mIOU\"]\n",
        "\n",
        "with open('/content/val_metrics.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # write the data\n",
        "    writer.writerow(['Baseline', np.nanmean(baseline_APs), b_ious])\n",
        "    writer.writerow(['DelftBikes (LR=0.001, ep=100)', np.nanmean(m1_APs), m1_ious])\n",
        "    writer.writerow(['DelftBikes (LR=0.0001, ep=100)', np.nanmean(m2_APs), m2_ious])\n"
      ],
      "metadata": {
        "id": "Sh_H6vN3kX3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visual Examples"
      ],
      "metadata": {
        "id": "2MEXO1keB2NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, _, gt_class_id, gt_bbox, gt_mask = \\\n",
        "    modellib.load_image_gt(valbikes, config, 10)\n",
        "\n",
        "r = model_LR0001.detect([image], verbose=1)[0]\n",
        "\n",
        "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
        "                            coco_class_names, r['scores'])"
      ],
      "metadata": {
        "id": "1KfNeVUnB1fX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}