{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV Bike Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96Asch/cv-bicycle-detection/blob/build_colab/CV_Bike_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nGiIgx6PQR_i"
      },
      "cell_type": "markdown",
      "source": [
        "#Bike detection using a Mask RCNN pretrained model\n",
        "In this notebook we fine tune a Mask RCNN model, trained on the MSCOCO dataset, on our own created bike dataset. This dataset has already been pre-generated and is stored on a publicly accesible drive.\n",
        "\n",
        "You can run the notebook in sequence except for when downgrading Keras. More instruction on this will be below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tpVxDUr2Pzgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywU9FM5qn6Wx"
      },
      "cell_type": "markdown",
      "source": [
        "#Install required packages\n",
        "\n",
        "Here we clone the Mask RCNN repo and install the requirements from the given requirements.txt file"
      ]
    },
    {
      "metadata": {
        "id": "39Wtm53bG7xW"
      },
      "cell_type": "code",
      "source": [
        "%cd\n",
        "  \n",
        "!git clone --quiet https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZY2h5buS9xb"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "\n",
        "!pip install -q PyDrive\n",
        "!pip install -r requirements.txt\n",
        "!pip install 'h5py<3.0.0'\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zZg68_koGU7"
      },
      "cell_type": "markdown",
      "source": [
        "#Download and extract dataset\n",
        "This downloads the COCO_Bikes dataset from Google Drive and extracts it into Mask_RCNN/dataset/"
      ]
    },
    {
      "metadata": {
        "id": "p3KxhAII1PDT"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "\n",
        "\n",
        "fileId = '14EAGfdRPSuIcrB3nmvXOfjtICFNALnMT'\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "if not os.path.exists('./dataset'):\n",
        "  os.makedirs('dataset')\n",
        "os.chdir('dataset')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "if not os.path.exists('./COCO_Bikes'):\n",
        "  fileName = fileId + '.zip'\n",
        "  downloaded = drive.CreateFile({'id': fileId})\n",
        "  downloaded.GetContentFile(fileName)\n",
        "  ds = ZipFile(fileName)\n",
        "  ds.extractall()\n",
        "  os.remove(fileName)\n",
        "  print('Extracted zip file ' + fileName)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the pretrained weights\n",
        "We fetch the MSCOCO pretrained weights of Mask RCNN\n"
      ],
      "metadata": {
        "id": "bn1XKNX_5cgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~\n",
        "\n",
        "modelId = '1Mos_U0jPpmNleZuLzGSTRTp7Ct4-PdAK'\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "if not os.path.exists('./pretrained.h5'):\n",
        "  downloaded = drive.CreateFile({'id': modelId})\n",
        "  downloaded.GetContentFile('pretrained.h5')"
      ],
      "metadata": {
        "id": "YA3ygqcriv1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Hjjc40apLpd"
      },
      "cell_type": "markdown",
      "source": [
        "#Configure the the Config and Dataset \n",
        "\n",
        "The Config class handles the configuration of the training.\n",
        "The Dataset class handles the import of our custom dataset"
      ]
    },
    {
      "metadata": {
        "id": "0SVHFnl5Hwe3"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/Mask_RCNN\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import sys\n",
        "\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "\n",
        "sys.path.append(os.path.join(\"/root/Mask_RCNN\", \"samples/coco/\"))  # To find local version\n",
        "import coco\n",
        "\n",
        "class DelftBikesConfig(Config):\n",
        "    NAME = \"delft-bikes\"\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 1 + 1  # background + bike\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    LEARNING_RATE = 0.0001\n",
        "  \n",
        "\n",
        "class COCODelftBikes(coco.CocoDataset):\n",
        "\n",
        "    def load(self, dataset_dir: str, subset: str) -> None:\n",
        "\n",
        "        path = os.path.join(dataset_dir, subset)\n",
        "        annotation = os.path.join(dataset_dir, f\"{subset}.json\")\n",
        "\n",
        "        coco = COCO(annotation)\n",
        "        class_ids = sorted(coco.getCatIds())\n",
        "        \n",
        "        image_ids = []\n",
        "        for id in class_ids:\n",
        "            image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "        # Remove duplicates\n",
        "        image_ids = list(set(image_ids))\n",
        "\n",
        "        self.add_class(\"coco\", 1, coco.loadCats(1)[0][\"name\"])\n",
        "        \n",
        "        \n",
        "          # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"coco\", image_id=i,\n",
        "                path=os.path.join(path, coco.imgs[i]['file_name']),\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "        Different datasets use different ways to store masks. This\n",
        "        function converts the different mask format to one format\n",
        "        in the form of a bitmap [height, width, instances].\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"coco\":\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
        "            return mask, np.ones([mask.shape[-1]], dtype=np.uint8)\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(coco.CocoDataset, self).load_mask(image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Check if the GPU is running"
      ],
      "metadata": {
        "id": "3gkpTG4K5pjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "4jeOnZoElFXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "Here we begin training the model, by first enabling Tensorboard in order to visualize each loss later on."
      ],
      "metadata": {
        "id": "j-R3wQ2G53Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "vZHNelGFlUuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Re-install the correct version of Keras\n",
        "For some reason, the built-in version of Keras gets reset when using the tensorflow version command. So we install the correct version: 2.2.5.\n",
        "You must reset the runtime to change the versions. Then re-run the config and dataset cell before continuing."
      ],
      "metadata": {
        "id": "6HFxbCsVCUkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.2.5\n",
        "!pip list | grep Keras"
      ],
      "metadata": {
        "id": "s1qemAvGJe-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the pretrained model and begin training using the dataset and config files specified before"
      ],
      "metadata": {
        "id": "7N2Tihblshzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "import mrcnn.model as modellib\n",
        "\n",
        "config = DelftBikesConfig()\n",
        "model_save_dir = os.path.join(\"/content\", \"exps\")\n",
        "pretrained_model_file = os.path.join(\"/root\", \"pretrained.h5\")\n",
        "data_set_dir = os.path.join(\"/root\", \"Mask_RCNN\", \"dataset\", \"COCO_Bikes\")\n",
        "\n",
        "with tf.device(device_name):\n",
        "    model = modellib.MaskRCNN(mode='training', config=config, model_dir=model_save_dir)\n",
        "\n",
        "assert os.path.exists(pretrained_model_file)\n",
        "    \n",
        "model.load_weights(pretrained_model_file, by_name=True, exclude=[ \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "dataset_train = COCODelftBikes()\n",
        "dataset_train.load(data_set_dir, 'train')\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = COCODelftBikes()\n",
        "dataset_val.load(data_set_dir, 'test')\n",
        "dataset_val.prepare()\n",
        "\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=100,\n",
        "            layers='all')"
      ],
      "metadata": {
        "id": "No4iRU46lXnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the losses using TensorBoard\n",
        "You should edit the path after --logdir and put the path to folder created by Mask RCNN which contains the tfevents file"
      ],
      "metadata": {
        "id": "MJG9ap27qs2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /root/exps/delft-bikes20220615T2029"
      ],
      "metadata": {
        "id": "q_y1HGpHpHWy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}